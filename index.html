<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="ICML Workshop on Security in Machine Learning">

  <title>Socially Responsible Machine Learning</title>

  <!-- Bootstrap core CSS -->
  <link href="bootstrap.min.css" rel="stylesheet">
</head>

<body>

<!-- Begin page content -->
<main role="main" class="container">
  <h1 class="mt-5">Socially Responsible Machine Learning
</h1>
  <p class="mb-0"><b>Date:</b> July 24, 2021</p>
  <p class="mb-0"><b>Location:</b> Virtual Only (co-located with <a href="https://icml.cc/">ICML 2021</a>)</p>
  <!-- <p class="mb-0"><b>Contact:</b> xxx (this will email all organizers)</p> -->
  <p>
    <i>Abstract</i>—Machine learning (ML) systems have been increasingly used in many applications, ranging from decision-making systems (e.g., automated resume screening and pretrial release tool) to safety-critical tasks (e.g., financial analytics and autonomous driving). While the hope is to improve decision-making accuracy and societal outcomes with these ML models, concerns have been incurred that they can inflict harm if not developed or used with care. It has been well-documented that ML models can:
    <ul>
      <li>  Inherit pre-existing biases and exhibit discrimination against already-disadvantaged or marginalized social groups.</li>
      <li> Be vulnerable to security and privacy attacks that deceive the models and leak the training data's sensitive information.
        Make hard-to-justify predictions with a lack of transparency.</li>
      <li>    Make hard-to-justify predictions with a lack of transparency.
      </li>
    </ul>

    For example, various commercial face recognition products were shown to have racial/gender bias, with female and darker-skinned people experiencing a much lower accuracy than the general population. In domains such as financial analytics and autonomous vehicles, ML models could be easily misled by carefully-crafted small perturbation or even natural perturbation. Therefore, it is essential to build socially responsible Machine Learning models that are fair, robust, private, transparent, and interpretable.
  </p>
    <p>
     
   This workshop aims to build connections by bringing together both theoretical and applied researchers from various communities (e.g., machine learning, fairness & ethics, security, privacy, etc.).  This workshop will focus on recent research and future directions for security problems in real-world machine learning and computer vision systems. We aim to bring together experts from the computer vision, security, and robust learning communities in an attempt to highlight recent work in this area as well as to clarify the foundations of secure machine learning. 
    
  </p>
  <p>
     <!-- Finally, we hope to chart out important directions for future work and cross-community collaborations, including computer vision, machine learning, security, and multimedia communities. -->
  </p>
  <!-- <h2>Sponsor</h2> -->
  <!-- <p></p> -->

  <h2>Schedule</h2>
  <p>The following is a tentative schedule and is subject to change prior to the workshop.</p>

  <table class="table table-sm">
    <tbody>
    <tr>
      <th scope="row">8:00am</th>
      <td>Opening Remarks</td>
      <td></td>
    </tr>

    <tr><th scope="row" colspan="3">Session 1:Interpretable Machine Learning Models</th></tr>
    <tr>
      <th scope="row">9:00am</th>
      <td>Invited Talk #1: </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">9:30am</th>
      <td>Contributed Talk #1: </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">10:00am</th>
      <td>Poster Spotlights #1: </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">10:00am</th>
      <td>Coffee Break </td>
    </tr>
    <tr><th scope="row" colspan="3">Session 2:Adversarial Examples in Physical World </th></tr>
    <tr>
      <th scope="row">10:30am</th>
      <td>Invited Talk #2: </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">11:00am</th>
      <td>Contributed Talk #2: </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">11:15am</th>
      <td>Invited Talk #3: </td>
      <td></td>
    <tr>
      <th scope="row">11:45am</th>
      <td>Poster Spotlights #2: </td>
      <td></td>
    </tr>
    </tr>
    <tr>
      <th scope="row">12:00pm</th>
      <td>Lunch </td>
    </tr>
    <tr><th scope="row" colspan="3">Session 3:Improve Model Robustness Against Adversarial Examples</th></tr>
    
    <tr>
      <th scope="row">1:15pm</th>
      <td>Invited Talk #4: </td>
      <td><a target="_blank"></a></td>
    </tr>
    <tr>
      <th scope="row">1:45pm</th>
      <td>Contributed Talk #3:</td>
      <td></td>
    </tr>


    <!-- <tr><th scope="row" colspan="3">2:00pm</th></tr> -->
    <tr>
      <th scope="row">2:00pm</th>
      <td>Poster Session followed by break</td>
      <td></td>
    </tr>

    <tr><th scope="row" colspan="3">Session 4: Adversarial Machine Learning in Autonomous Driving
</th></tr>
    <tr>
      <th scope="row">2:45pm</th>
      <td>Invited Talk #5: </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">3:15pm</th>
      <td>Contributed Talk #4: </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">3:30pm</th>
      <td>Invited Talk #6: </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">4:00pm</th>
      <td>Contributed Talk #5: </td>
      <td></td>
    </tr>
    </tbody>
  </table>
  
<h2>Important Dates</h2>
<ul>
  <li>Workshop paper submission deadline: 06/10/2021</li>
  <li>Notification to authors: 07/01/2021</li>
  <li>Camera ready deadline: 07/10/2021</li>
</ul>

	

<h2>Call For Papers</h2>
  <p class="mb-0"><b>Submission deadline:</b> June 10, 2021 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Notification sent to authors:</b> July 1, 2021 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Submission server:</b> <a href="https://easychair.org/cfp/AdvMLCV2019" target="_blank">https://easychair.org/cfp/AdvMLCV2019</a></p>
	
  <p>The workshop will include contributed papers. Based on the PC’s recommendation, each paper accepted to the workshop will be allocated either a contributed talk or poster presentation .</p>
  <p>We invite submissions on <b>any aspect of machine learning that relates to fairness & ethics, transparency, interpretability, security, and privacy</b>. This includes, but is not limited to:</p>

   <ul>
     <li>    The intersection between various pillars of trust: fairness, transparency, interpretability, privacy, robustness. 
     </li>
     <li>    The state-of-the-art research of trustworthy ML in applications
     </li>
     <li>    The possibility of adopting the most recent theory to inform practice guidelines for deploying trustworthy ML systems.  
     </li>
     <li>
       Providing insights about how we can automatically detect, verify, explain, and mitigate potential biases or privacy problems in existing models.
     </li>
     <li>    Understanding the tradeoffs or costs of achieving different goals in reality. 
     </li>
     <li>
       Explaining the social impacts of the machine learning bias.   

     </li>
   </ul>

  <h2>Organizing Committee</h2>
  <div class="row justify-content-around">
    <!-- <div class="col-lg-1"></div> -->
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/boli.png" width="150px" height="150px">
      <p style="width:150px" >Bo Li<br /></p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/dawn.png" width="150px" height="150px">
      <p style="width:150px" >Dawn Song</p>
    </div>
    <!-- </div> -->
      <div class="col-md-1">
      <img class="rounded-circle" src="imgs/chaowei.jpg" width="150px" height="150px">
      <p style="width:150px" >Chaowei Xiao</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/xueru.png" width="150px" height="150px">
      <p style="width:150px" >Xueru Zhang</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/raquel.jpeg" width="150px" height="150px">
      <p style="width:150px" >Rauel Urtasun </p>
    </div>

  </div>
  <div class="row justify-content-around">
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/cihang.jpeg" width="150px" height="150px">
      <p style="width:150px" >Cihang Xie</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/xinyun.jpeg" width="150px" height="150px">
      <p style="width:150px" >Xinyun Chen</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/mingyan.jpeg" width="150px" height="150px">
      <p style="width:150px" >Mingyan Liu</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/anima.png" width="150px" height="150px">
      <p style="width:150px" >Anima Anandkumar</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/jieyu.jpeg" width="150px" height="150px">
      <p style="width:150px" >Jieyu Zhao</p>
    </div>
    

    <!-- <div class="col-lg-1"></div> -->
  </div>
<h2>Program Committee</h2>
<li>Bhavya Khailkhura (Lawrence Livermore National Lab)</li>
<li>Catherine Olsson (Google Brain)</li>
<li>Chaowei Xiao (University of Michigan)</li>
<li>David Evans (University of Virginia)</li>
<li>Dimitris Tsipras (Massachusetts Institute of Technology)</li>
<li>Earlence Fernandes (University of Washington)</li>
<li>Eric Wong (Carnegie Mellon University)</li>
<li>Fartash Faghri (University of Toronto)</li>
<li>Florian Tramer (Stanford University)</li>
<li>Hadi Abdullah (University of Florida)</li>
<li>Hao Su (UCSD)</li>
<li>Jonathan Uesato (DeepMind)</li>
<li>Karl Ni (In-Q-Tel)</li>
<li>Kassem Fawaz (University of Wisconsin-Madison)</li>
<li>Kathrin Grosse (CISPA)</li>
<li>Krishna Gummadi (MPI-SWS)</li>
<li>Matthew Wicker (University of Georgia)</li>
<li>Nathan Mundhenk (Lawrence Livermore National Lab)</li>
<li>Nicholas Carlini (Google Brain)</li>
<li>Nicolas Papernot (Google Brain and University of Toronto)</li>
<li>Octavian Suciu (University of Maryland)</li>
<li>Pin-Yu Chen (IBM)</li>
<li>Pushmeet Kohli (DeepMind)</li>
<li>Shreya Shankar (Stanford University)</li>
<li>Suman Jana (Columbia University)</li>
<li>Varun Chandrasekaran (University of Wisconsin-Madison)</li>
<li>Xiaowei Huang (Liverpool University)</li>
<li>Yanjun Qi (University of Virginia)</li>
<li>Yigitcan Kaya (University of Maryland)</li>
<li>Yizheng Chen (Georgia Tech)</li> 

</body></html>
