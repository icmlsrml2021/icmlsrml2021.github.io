<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="ICML Workshop on Security in Machine Learning">

  <title>Socially Responsible Machine Learning</title>

  <!-- Bootstrap core CSS -->
  <link href="bootstrap.min.css" rel="stylesheet">
</head>

<body>

<!-- Begin page content -->
<main role="main" class="container">
  <h1 class="mt-5 cneter">Socially Responsible Machine Learning
</h1>
  <p class="mb-0"><b>Date:</b> July 24, 2021</p>
  <p class="mb-0"><b>Location:</b> Virtual Only (co-located with <a href="https://icml.cc/">ICML 2021</a>)</p>
  <p class="mb-0"><b>Update:</b></p
  <p> 
    <ul>
      <li>
        We've sent out the results for the papers. Check all the <a href="paper.html">accepted paper</a>.
      </li>
    </ul>
  </p>
  <!-- <p class="mb-0"><b>Contact:</b> xxx (this will email all organizers)</p> -->
  <p>
    <i>Abstract</i>—Machine learning (ML) systems have been increasingly used in many applications, ranging from decision-making systems (e.g., automated resume screening and pretrial release tool) to safety-critical tasks (e.g., financial analytics and autonomous driving). While the hope is to improve decision-making accuracy and societal outcomes with these ML models, concerns have been incurred that they can inflict harm if not developed or used with care. It has been well-documented that ML models can:
    <ul>
      <li>  Inherit pre-existing biases and exhibit discrimination against already-disadvantaged or marginalized social groups.</li>
      <li> Be vulnerable to security and privacy attacks that deceive the models and leak the training data's sensitive information.
       </li>
      <li>    Make hard-to-justify predictions with a lack of transparency.
      </li>
    </ul>

    For example, various commercial face recognition products were shown to have racial/gender bias. In domains such as financial analytics and autonomous vehicles, ML models could be easily misled by carefully-crafted small perturbation or even natural perturbation. Therefore, it is essential to build socially responsible Machine Learning models that are fair, robust, private, transparent, and interpretable.
  </p>
    <p>
     
   This workshop aims to build connections by bringing together both theoretical and applied researchers from various communities (e.g., machine learning, fairness & ethics, security, privacy, etc.).  This workshop will focus on recent research and future directions for socially responsible machien learning  problems in real-world machine learning systems. We aim to bring together experts from different communities in an attempt to highlight recent work in this area as well as to clarify the foundations of socially responsible machine learning. 

    
  </p>
  <p>
     <!-- Finally, we hope to chart out important directions for future work and cross-community collaborations, including computer vision, machine learning, security, and multimedia communities. -->
  </p>
  <!-- <h2>Sponsor</h2> -->
  <!-- <p></p> -->

  <h2>Schedule</h2>
  <p>The tentative schedule is subject to change prior to the workshop. Time Zone: US Eastern Time Zone (UTC-05:00)  </p>
  <p>Accepted paper can be found <a href="paper.html">here</a>.</p>

  <table class="table table-sm">
    <tbody>
    <tr>
      <th scope="row">8:45 - 9:00</th>
      <td>Anima Anandkumar-Opening Remarks</td>
      <td></td>
    </tr>

    <!-- <tr><th scope="row" colspan="3">Session 1:Interpretable Machine Learning Models</th></tr> -->
    <tr>
      <th scope="row">9:00 - 9:40 </th>
      <td>Jun Zhu, </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">9:40 - 10:20</th>
      <td>Olga Russakovsky, Revealing, quantifying, analyzing and mitigating bias in visual recognition </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">10:20 - 11:00</th>
      <td>Pin-yu Chen: Adversarial Machine Learning for Good </td>
      <td></td>
    </tr>
    <!-- <tr>
      <th scope="row">10:40 - 11:00</th>
      <td>Contributed Talk #2: Auditing AI models for Verified Deployment under Semantic Specifications </td>
    </tr> -->
    <!-- <tr>
      <th scope="row">10:20 - 10:40</th>
      <td>Contributed Talk #1: FERMI: Fair Empirical Risk Minimization Via Exponential Rényi Mutual Information </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">10:40 - 11:00</th>
      <td>Contributed Talk #2: Auditing AI models for Verified Deployment under Semantic Specifications </td>
    </tr> -->
    <tr><th scope="row" colspan="3">Coffee Break</th></tr>
    <!-- <tr><th scope="row" colspan="3">Session 2:Adversarial Examples in Physical World </th></tr> -->
    <tr>
      <th scope="row">11:10 - 11:50a</th>
      <td>Aaron Roth: Better Estimates of Prediction Uncertainty</td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">11:50 - 12:30</th>
      <td>Nicolas Papernot, Workshop on Socially Responsible Machine Learning </td>
      <td></td>
    </tr>
    <tr><th scope="row" colspan="3"> Lunch </th></tr>
    <tr>
      <th scope="row">13:30-13:50</th>
      <td>Contributed Talk #1: Machine Learning API Shift Assessments </td>
      <td></td>
    <tr>
      <th scope="row">13:50-14:30</th>
      <td>Tatsu Hashimoto: Not all uncertainty is noise: machine learning with confounders and inherent disagreements </td>
      <td></td>
    </tr>
    </tr>
    <tr>
      <th scope="row">14:30-15:10</th>
      <td>Jun-Yan Zhu, Understanding and Rewriting GANs </td>
    </tr>
    <tr><th scope="row" colspan="3">Break</th></tr>
    
    <tr>
      <th scope="row">15:20-16:00</th>
      <td>Kai-Wei Chang, Societal Bias in Language Generation </td>
      <td><a target="_blank"></a></td>
    </tr>
    <tr>
      <th scope="row">16:00 - 16:40 </th>
      <td>Yulia Tsvetkov, Towards Factual Consistency in Language Generation </td>
      <td></td>
    </tr>

    <tr>
      <th scope="row">16:40-17:00</th>
      <td>Contributed Talk #2: Do Humans Trust Advice More if it Comes from AI? An Analysis of Human-AI Interactions</td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">17:00 - 17:20</th>
      <td>Contributed Talk #3: FERMI: Fair Empirical Risk Minimization Via Exponential Rényi Mutual Information </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">17:20 - 17:40</th>
      <td>Contributed Talk #4: Auditing AI models for Verified Deployment under Semantic Specifications </td>
    </tr>
    </tbody>
</table>  
 

  <h2>Organizing Committee</h2>
  <div class="row justify-content-around">
    <!-- <div class="col-lg-1"></div> -->
    <!-- </div> -->
      <div class="col-md-1">
      <img class="rounded-circle" src="imgs/chaowei.jpeg" width="150px" height="150px">
      <p style="width:150px" >Chaowei Xiao</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/xueru.png" width="150px" height="150px">
      <p style="width:150px" >Xueru Zhang</p>
    </div>  
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/jieyu.jpeg" width="150px" height="150px">
      <p style="width:150px" >Jieyu Zhao</p>
    </div>
  </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/cihang.jpeg" width="150px" height="150px">
      <p style="width:150px" >Cihang Xie</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/xinyun.jpeg" width="150px" height="150px">
      <p style="width:150px" >Xinyun Chen</p>
    </div>


  <h2>Senior Organizing Committee</h2>
    <div class="row justify-content-around">
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/anima.png" width="150px" height="150px">
      <p style="width:150px" >Anima Anandkumar</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/boli.png" width="150px" height="150px">
      <p style="width:150px" >Bo Li<br /></p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/mingyan.jpeg" width="150px" height="150px">
      <p style="width:150px" >Mingyan Liu</p>
    </div>

    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/dawn.png" width="150px" height="150px">
      <p style="width:150px" >Dawn Song</p>
    </div>
   
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/raquel.jpeg" width="150px" height="150px">
      <p style="width:150px" >Raquel Urtasun </p>
    </div>
    <!-- <div class="col-lg-1"></div> -->
  </div>

  
<h2>Program Committee</h2>
<li>Aishan Liu (Beihang University)</li>
<li>Anqi Liu (Caltech)</li>
<li>Akshayvarun	Subramanya (UMBC)</li>
<li>Alexandra	 Chouldechova (CMU)</li>
<li>Aniruddha		Saha(University of Maryland Baltimore County)</li>
<li>Anshuman		Suri (University of Virginia)</li>
<li>Bo Ji (Virginia Tech)</li>
<li>Boxin		Wang (University of Illinois at Urbana-Champaign)</li>
<li>Chen		Zhu (University of Maryland)</li>
<li>Chirag		Agarwal	(Harvard University)</li>
<li>Chulin		Xie	(University of Illinois at Urbana-Champaign) </li>
<li>Hongyang		Zhang	(TTIC)</li>
<li>Huan		Zhang	(UCLA) </li>
<li>Jamie		Hayes (Google DeepMind)	</li>
<li>Jia		Liu	(Ohio State University)	</li>
<li>Jiachen		Sun	(University of Michigan) </li>
<li>Josiah		Wong (Stanford University)	</li>	
<li>Juba		Ziani (University of Pennsylvania)</li>	
<li>Junheng		Hao		(UCLA)	</li>	
<li>Kexin		Rong	 	(Stanford University)	</li>	
<li>Kun		Jin	 (University of Michigan, Ann Arbor)</li>	
<li>Maura		Pintor 	(University of Cagliari)</li>		
<li>Mohammad Mahdi	 	Khalili	 	(University of Delaware)</li>	
<li>Muhammad		Awais	 (Kyung-Hee University)</li>	
<li>Nataniel		Ruiz	 	(Boston University)</li>	
<li>Parinaz		Naghizadeh 	(Ohio State University)</li>		
<li>Rajkumar		Theagarajan	 	(University of California, Riverside)</li>	
<li>Sravanti		Addepalli	 	(Indian Institute of Science)</li>	
<li>Sunipa		Dev	 	(University of Utah)</li>	
<li>Wenxiao		Wang	 	(Tsinghua University)</li>		
<li>Won		Park	 (University of Michigan)</li>	
<li>Xinchen		Yan	 (Uber ATG)</li>	
<li>Xingjun		Ma	 (Deakin University)	</li>	
<li>Xinlei		Pan	 	(UC Berkeley)</li>	
<li>Xinwei		Zhao	 	(Drexel University)	</li>	
<li>Xueru	 	Zhang	 (University of Michigan)</li>	
<li>Yingwei		Li	 	(Johns Hopkins University)</li>	
<li>Yizhou	Sun	 	(UCLA)</li>	
<li>Yulong		Cao	 (University of Michigan, Ann Arbor) </li>	
<li>Yuzhe		Yang	 (MIT)</li>	
<li>Zelun		Luo	 	(Stanford University)</li>	
<li>Zhiding		Yu	 	(NVIDIA)</li>	

<!-- <h2 id="accepted">Accepted Paper</h2> -->


 
<h2>Important Dates</h2>
<ul>
  <li>Workshop paper submission deadline: 06/10/2021</li>
  <li>Notification to authors: 07/10/2021</li>
  <li>Camera ready deadline: 07/15/2021</li>
</ul>

	

<h2>Call For Papers</h2>
  <p class="mb-0"><b>Submission deadline:</b> June 10, 2021 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Notification sent to authors:</b> July 10, 2021 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Submission server:</b> <a href="https://cmt3.research.microsoft.com/ICMLSRML2021/" target="_blank">https://cmt3.research.microsoft.com/ICMLSRML2021/</a></p>
	
  <p>The workshop will include contributed papers. The workshop will be completely virtual. We will update the details later. 
     <!-- Based on the PC’s recommendation, each paper accepted to the workshop will be allocated either a contributed talk or poster presentation . -->
   </p>
  <p>We invite submissions on <b>any aspect of machine learning that relates to fairness, ethics, transparency, interpretability, security, and privacy</b>. This includes, but is not limited to:</p>

   <ul>
     <li>    The intersection between various pillars of trust: fairness, transparency, interpretability, privacy, robustness. 
     </li>
     <li>    The state-of-the-art research of trustworthy ML in applications
     </li>
     <li>    The possibility of adopting the most recent theory to inform practice guidelines for deploying trustworthy ML systems.  
     </li>
     <li>
       Providing insights about how we can automatically detect, verify, explain, and mitigate potential biases or privacy problems in existing models.
     </li>
     <li>    Understanding the tradeoffs or costs of achieving different goals in reality. 
     </li>
     <li>
       Explaining the social impacts of the machine learning bias.   

     </li>
   </ul>
   <p class="mb-0"><b>Submission Format: </b> We welcome submissions up to 4 pages in ICML Proceedings format (double-blind), excluding references and appendix. <a href="https://media.icml.cc/Conferences/ICML2021/Styles/icml2021_style.zip">Style files</a>  and <a href="https://media.icml.cc/Conferences/ICML2021/Styles/example_paper.pdf">an example  </a>paper are available.   We allow an unlimited number of pages for references and supplementary material, but reviewers are not required to review the supplementary material.  Unless indicated by the authors, we will provide PDFs of all accepted papers on https://icmlsrml2021.github.io.  There will be no archival proceedings.
  
    We are using CMT3 to manage submissions.
    </p>
    <!-- <p class="mb-0"><b>Ethics: </b> We ask that authors think about the broader impact and ethical considerations of their work. For example, authors may consider whether there is potential use for the data or methods to create or exacerbate unfair bias. Authors are not required to have a broader impact statement in their paper, but will be asked to submit one paragraph (3-4 sentences) as a separate field in OpenReview at the time of submission. See this guide for help with the statement. Reviewers will be asked to consult the ICLR 2021 code of ethics when reviewing submissions. Reviewers cannot reject papers based on ethical considerations, but in very rare cases, the workshop organizers may reject papers that blatantly violate the code of ethics (e.g., if the primary application directly causes harm or injury). 
    </p> -->
    
    
    
   </p>

</body></html>
